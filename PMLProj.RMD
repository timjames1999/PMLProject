---
title: "Practical Machine Learning Project"
author: "Jerin Timothy James"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways (More information is available from the website here: http://groupware.les.inf.puc-rio.br/har).  

The goal of this project is to build a machine learning algorithm to predict activity quality (classe) from activity monitors.  The classe will be in 5 factors: A, B, C, D, E.

### Exploratory Data Analysis

First we need to load the required libraries and data set (both training and test data already seperated) and examine its structure with the below code:

```{r} 
#Load needed libraries and set seed
library(caret)
library(ggplot2)
library(randomForest)
set.seed(1999)

#Download and read data file once to save time
trainurl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainfile <- "training.csv"
testfile  <- "testing.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainfile)) {
  download.file(trainurl, destfile=trainfile, method="curl")
}
if (!file.exists(testfile)) {
  download.file(testurl, destfile=testfile, method="curl")
}

train <- read.csv("training.csv", na.strings=c("NA","#DIV/0!",""))
test <- read.csv("testing.csv", na.strings=c("NA","#DIV/0!",""))

```

Next we do some basic exploration of the (training) data with a look at the structure and do some basic data clean up with the below code: 

```{r} 
#Initial look at data
str(train)

#Remove variables with majority NAs (>50%)
train_SUB <- train
for (i in 1:length(train)) {
  if (sum(is.na(train[ , i])) / nrow(train) >= .50) {
    for (j in 1:length(train_SUB)) {
      if (length(grep(names(train[i]), names(train_SUB)[j]))==1) {
        train_SUB <- train_SUB[ , -j]
      }
    }
  }
}

#Remove first 7 columns that are not used as predictors
train_SUB2 <- train_SUB[,8:length(train_SUB)]

```

Next we take a look at a Feature Plot to look for any obvious patterns or outliers of interest with the below coed:

```{r} 
featurePlot(x=train[, 150:159], y = train$classe, plot = 'pairs')
```

### Model Creation

I decided to use the Random Forest method in the Caret package (mainly because this was the method most covered in the lectures) with the below code:

```{r} 
#Random Forest Model
modFit <- train(classe ~., method = "rf", trControl=trainControl(method = "cv", number = 4), data = train_SUB2)
print(modFit)
```

### Error
Based on the Random Forest function output above, you can see we use a 4 fold cross validation which resulted in a 99.4% accuracy (in-sample).  This was accomplished whit an mtry (number of randomly drawn candidate variables out of which each split is selected when growing a tree) of 27.

### Validation with Quiz (Test) Data

The class quiz was available to use with the above code and it returned 100% correct data with the below code:

```{r} 
#Prediction on Quiz Test Data
predict_FINAL <- predict(modFit, test, type = "raw")
print(predict_FINAL)
```

### Conclusion

Based on the posted error rates in the Random Forest model as well as the 100% Quiz data validation, this model appears to be accurate to a high degree.